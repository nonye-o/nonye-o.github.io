{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf289090",
   "metadata": {},
   "source": [
    "# NumPy Neural Network Implementation From Scratch\n",
    "This Python script is a highly generalizable implementation of a Multi-Layer Perceptron (MLP) network.\n",
    "\n",
    "- Activation functions and cost functions can be varied and new ones can be defined in the relevant section by also defining the corresponding derivative and adding them to their respective dictionaries in the same manner as the rest.\n",
    "- Network weights can be initialized as one of two choices: uniform random weight initialization $U(-1,1)$ or He Uniform weight initialization.\n",
    "- L2-Regularization can be applied in training.\n",
    "\n",
    "To initialize a neural network (neural) instance, you need the following arguments:\n",
    "- a list of layer sizes in order, starting from the number of neurons in the input layer;\n",
    "- a list of activation functions in order starting from the first hidden layer; and\n",
    "- (optional) a boolean to determine whether He Uniform weight initialization is applied. If True, He Uniform weight initialization will be applied to each layer, otherwise $U(-1,1)$ will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb00c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all dependencies\n",
    "import numpy as np\n",
    "import pandas as pd             # Used to load the data\n",
    "import matplotlib.pyplot as plt # Used only to plot relevant graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a2f90",
   "metadata": {},
   "source": [
    "# Define Cost Functions with Derivatives\n",
    "\n",
    "The cost function is the function that ultimately allows us determine which parameter modifications lead to better predictions from our neural network. A key assumption with gradient descent is that the cost function is differentiable. Without this assumption, we may need to precompute the resultant cost after making a prospective modification to any given parameter, and then modify the parameter accordingly, which would be computationally expensive. Thus, we should choose a cost function for which the derivative can be found.\n",
    "\n",
    "It will be more practical to define the loss functions here so below we will define some loss functions and their associated derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6bc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.        ]\n",
      " [1.50466654 0.2048005  2.07643925 1.13485422 0.41576086]]\n",
      "[[-0.         -0.         -0.         -0.         -0.        ]\n",
      " [-4.50265193 -1.2272802  -7.97601767 -3.11072003 -1.5155234 ]]\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(y, y_hat):\n",
    "    return 0.5*(y_hat - y)**2\n",
    "\n",
    "def mse_loss_d(y, y_hat):\n",
    "    return (y_hat - y)\n",
    "\n",
    "def binary_ce_loss(y, y_hat):\n",
    "    '''\n",
    "    This function returns the binary cross entropy loss of two ndarrays with shape (n,m). Each row will consist of the loss values corresponding to a single \n",
    "    data point.\n",
    "\n",
    "    Args:\n",
    "        y       :   ndarray with shape (n,m) containing labels consisting of 0s and 1s. The number of rows n should correspond to the number of data points.\n",
    "        y_hat   :   ndarray with shape (n,m) containing sigmoid value output predictions. The number of rows n should correspond to the number of data points.\n",
    "    '''\n",
    "    return y*np.log(y_hat+ 10**(-8)) + (1-y)*np.log(1-y_hat+ 10**(-8))\n",
    "\n",
    "def binary_ce_loss_d(y, y_hat):\n",
    "    '''\n",
    "    This function returns the derivative of the binary cross entropy loss of two ndarrays with shape (n,m). Each row will consist of the derivative of the \n",
    "    loss values corresponding to a single data point.\n",
    "\n",
    "    Args:\n",
    "        y       :   ndarray with shape (n,m) containing labels consisting of 0s and 1s. The number of rows n should correspond to the number of data points.\n",
    "        y_hat   :   ndarray with shape (n,m) containing sigmoid value output predictions. The number of rows n should correspond to the number of data points.\n",
    "    '''\n",
    "    return y/(y_hat + 10**(-8)) + (1-y)/(y_hat - 1 - 10**(-8))\n",
    "\n",
    "def categorical_ce_loss(y, y_hat):\n",
    "    return -y*np.log(y_hat+10**(-8))\n",
    "\n",
    "def categorical_ce_loss_d(y, y_hat):\n",
    "    return -y/(y_hat+10**(-8))\n",
    "\n",
    "# Create dictionary to store loss functions and their derivatives\n",
    "loss_function_dict = {\n",
    "    'mean squared error': [mse_loss, mse_loss_d],\n",
    "    'binary cross entropy': [binary_ce_loss, binary_ce_loss_d],\n",
    "    'categorical cross entropy': [categorical_ce_loss, categorical_ce_loss_d]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157cb62",
   "metadata": {},
   "source": [
    "# Define Activation Functions with Derivatives\n",
    "\n",
    "Activation functions are the reason neural networks are not simply linear models, thus we must include them in our neural network.\n",
    "\n",
    "There is also a key assumption that is made with gradient descent - that the derivative of the activation functions exists. This assumption is allows us to use multivariate calculus to determine the derivative of each of the network's parameters with respect to the cost function. Without this assumption, we may need to precompute the resultant cost after making a prospective modification to any given parameter, and then modify the parameter accordingly, which would be computationally expensive. Thus, we should choose activation functions for which the derivative can be found.\n",
    "\n",
    "Below we will define some activation functions and their associated derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3215260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions and their derivatives\n",
    "def linear(zz):\n",
    "    '''\n",
    "    This function returns the linear output values of a given input array. This is just the identity activation function.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return zz\n",
    "\n",
    "def linear_d(zz):   # Derivative\n",
    "    '''\n",
    "    This function returns the derivative of the linear output values of a given input array. \n",
    "    In other words, this function returns 1.0.\n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return 1.0\n",
    "\n",
    "def relu(zz):\n",
    "    '''\n",
    "    This function returns the ReLU values of a given input array.    \n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return np.maximum(zz, 0)\n",
    "\n",
    "def relu_d(zz):     # Derivative\n",
    "    '''\n",
    "    This function returns the derivative of the ReLU values of a given input array.    \n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return (zz>=0)\n",
    "\n",
    "# Here a Leaky ReLU function is defined. The constant \"leak\" will be the gradient of the output when the output is below 0.\n",
    "leak = 0.1\n",
    "def leaky_relu(zz):\n",
    "    '''\n",
    "    This function returns the Leaky ReLU values of a given input array. This should \n",
    "    be an alternative to the ReLU function since the ReLU function may cause gradients \n",
    "    to become permanently 0 when the weighted sum is below 0.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return zz*(zz >= 0) + (leak*zz)*(1 - (zz >= 0))\n",
    "\n",
    "def leaky_relu_d(zz):\n",
    "    '''\n",
    "    This function returns the derivative of the Leaky ReLU values of a given input array.    \n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return (zz >= 0) + (leak)*(1 - (zz >= 0))\n",
    "\n",
    "\n",
    "def sigmoid(zz):\n",
    "    '''\n",
    "    This function returns the sigmoid values of a given input array.    \n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return 1/(1+np.exp(-zz))\n",
    "\n",
    "def sigmoid_d(zz):  # Derivative\n",
    "    '''\n",
    "    This function returns the derivative of the sigmoid values of a given input array.    \n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return np.exp(-zz)/(1+np.exp(-zz))**2\n",
    "\n",
    "def softmax(zz):\n",
    "    '''\n",
    "    This function returns the softmax values of a given input array.\n",
    "    The input should have shape (n,m) where n is the number of data rows and m is the number of output neurons.\n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values. Should have shape (n,m) where \n",
    "                n is the number of data rows and m is the number of output neurons.\n",
    "    '''\n",
    "    zz = zz.reshape(zz.shape[0], -1)    # BEWARE: this will turn 1D arrays of length n into a column vector with shape (n, 1), so if you input a single neural network output, make sure it is already a 2D array\n",
    "    zz_max = np.max(zz, axis = 1).reshape(zz.shape[0], -1)\n",
    "    return np.exp(zz - zz_max)/np.sum(np.exp(zz - zz_max), axis = 1).reshape(-1, 1)\n",
    "\n",
    "def softmax_d(zz):  # Derivative\n",
    "    '''\n",
    "    This function returns the derivative of the softmax values of a given input array.\n",
    "    The input should have shape (n,m) where n is the number of data rows and m is the number of output neurons.\n",
    "    \n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values. Should have shape (n,m) where \n",
    "                n is the number of data rows and m is the number of output neurons.\n",
    "    '''\n",
    "    softmax_output = softmax(zz)\n",
    "    \n",
    "    # This is the jacobian matrix and it will be used differently to derivatives of the other functions since \n",
    "    # those functions were defined element-wise while the softmax is defined on a vector, i.e. the other \n",
    "    # activation functions go from R to R while softmax goes from R^m to R^m\n",
    "    jacobian = np.array([[[(softmax_output[nn, jj]*(1-softmax_output[nn, jj]) if jj == ii else -softmax_output[nn, ii]*softmax_output[nn, jj]) for jj in range(zz.shape[1])] for ii in range(zz.shape[1])] for nn in range(zz.shape[0])])\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# Here an exponential activation function is defined. The output will be e^(k*zz) where k=exp_const\n",
    "exp_const = 0.01\n",
    "def exponential(zz):\n",
    "    '''\n",
    "    This function returns the exponential output values for a given input array.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return np.exp(exp_const*zz)\n",
    "\n",
    "def exponential_d(zz):\n",
    "    '''\n",
    "    This function returns the exponential output derivative values for a given input array.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return exp_const*np.exp(exp_const*zz)\n",
    "\n",
    "def tanh(zz):\n",
    "    '''\n",
    "    This function returns the hyperbolic tangent output values for a given input array.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    #zz_max = np.max(zz, axis = 1).reshape(zz.shape[0], -1)\n",
    "    return (np.exp(zz) - np.exp(-zz))/(np.exp(zz) + np.exp(-zz))\n",
    "\n",
    "def tanh_d(zz):\n",
    "    '''\n",
    "    This function returns the hyperbolic tangent output derivative values for a given input array.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return (2/(np.exp(zz) + np.exp(-zz)))**2\n",
    "\n",
    "# Here an exponential linear unit activation function is defined with alpha equal to alpha_elu\n",
    "alpha_elu = 1\n",
    "def elu(zz):\n",
    "    '''\n",
    "    This function returns the exponential linear unit output values for a given input array.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return zz*(zz >= 0) + alpha_elu*(np.exp(zz) - 1)*(zz < 0)\n",
    "\n",
    "def elu_d(zz):\n",
    "    '''\n",
    "    This function returns the exponential linear unit output derivative values for a given input array.\n",
    "\n",
    "    Args:\n",
    "        zz  :   ndarray containing the pre-activation (or weighted sum) values.\n",
    "    '''\n",
    "    return 1*(zz >= 0) + alpha_elu*np.exp(zz)*(zz < 0)\n",
    "\n",
    "# Create dictionary to store activation functions with their derivatives\n",
    "# This dict will store a list containing the activation function at index 0, and the derivative at index 1\n",
    "activation_function_dict = {\n",
    "    'linear': [linear, linear_d],\n",
    "    'relu': [relu, relu_d],\n",
    "    'leaky relu': [leaky_relu, leaky_relu_d],\n",
    "    'sigmoid': [sigmoid, sigmoid_d],\n",
    "    'softmax': [softmax, softmax_d],\n",
    "    'exponential': [exponential, exponential_d],\n",
    "    'tanh': [tanh, tanh_d],\n",
    "    'elu': [elu, elu_d]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81cd4a",
   "metadata": {},
   "source": [
    "# Create Neural Network\n",
    "\n",
    "We will initialize a neural network given the layer lengths and the activation functions. This will fully determine the shapes of the weight and bias arrays. The weights will be initialized randomly and the biases will be initialized to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c067414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to initialize parameters\n",
    "def create_parameters(layer_dims):\n",
    "    '''\n",
    "    This function will input a list containing layer dimensions (including the input layer size), and will output \n",
    "    randomly intialized weights in the range [-1, 1) at index 0 of the output, and biases intialized to 0 at index 1 \n",
    "    of the output.\n",
    "\n",
    "    Args:\n",
    "        layer_dims  :   A list-like object containing integers representing the number of neurons in each layer.\n",
    "                        The number of input neurons should be included at the first index and all other indices \n",
    "                        should be ordered accordingly following. The input layer is considered \"layer 0\".\n",
    "    '''\n",
    "    weight_dict = {}\n",
    "    bias_dict = {}\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "        weight_dict[f'layer {i + 1}'] = (np.random.rand(layer_dims[i], layer_dims[i + 1]) - 0.5)*2\n",
    "        bias_dict[f'layer {i + 1}'] = np.zeros((1, layer_dims[i + 1]), dtype = float)\n",
    "\n",
    "    return weight_dict, bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural:\n",
    "    def __init__(self, layers, activation_function_list, he_uniform=False):\n",
    "        '''\n",
    "        Args:\n",
    "            layers                      :   A list-like object containing integers representing the number of neurons in each layer.\n",
    "                                            The number of input neurons should be included at the first index and all other indices \n",
    "                                            should be ordered accordingly following. The input layer is considered \"layer 0\".\n",
    "            activation_function_list    :   A list-like object containing the sublists. The sublists contain the activation functions \n",
    "                                            at subindex 0 and contain the derivatives of the activation functions at subindex 1. The \n",
    "                                            sublist at index i contains activation function and derivative for layer i + 1, thus, \n",
    "                                            first element of activation function list corresponds to layer 1 of the neural network.\n",
    "                                            activation_function_list should have 1 less element than the layers list.\n",
    "            he_uniform                  :   A boolean determining whether or not the parameters should be initialized using a He Uniform \n",
    "                                            distribution.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = create_parameters(layers)\n",
    "        self.activation_functions = [activation_function_dict[act] for act in activation_function_list]\n",
    "        self.activation_function_name_list = activation_function_list\n",
    "\n",
    "        # Initialize arrays to store history\n",
    "        self.cost_history = np.array([], dtype = float)\n",
    "        self.accuracy_history = np.array([], dtype = float)\n",
    "        self.cv_cost_history = np.array([], dtype = float)\n",
    "        self.cv_accuracy_history = np.array([], dtype = float)\n",
    "        self.weight_update_max_history = np.zeros(shape=(0, len(self.layers) - 1), dtype = float)\n",
    "        self.weight_update_mean_history = np.zeros(shape=(0, len(self.layers) - 1), dtype = float)\n",
    "\n",
    "        # Apply He Uniform Initialization to the parameters if selected\n",
    "        if he_uniform:\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                self.weights[f'layer {i}'] *= np.sqrt(6/self.weights[f'layer {i}'].shape[0])\n",
    "\n",
    "        \n",
    "\n",
    "    def __call__(self, input_layer):\n",
    "        '''\n",
    "        This function computes the output of the neural network given an input layer.\n",
    "\n",
    "        Args:\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                                as an input of the actual neural network.\n",
    "        '''\n",
    "        x = input_layer\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = np.matmul(x, self.weights[f'layer {i + 1}']) + self.biases[f'layer {i + 1}']\n",
    "            x = self.activation_functions[i][0](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def call_with_parameters(self, input_layer, weights, biases):\n",
    "        '''\n",
    "        This function computes the output of the neural network given an input layer, weights, and biases. The weights \n",
    "        do not necessarily have to be the same as the model's weights, but there must be the same number of weight \n",
    "        matrices and these weight matrices must have the same shape as the model's corresponding weight matrices. The \n",
    "        same is true for the biases.\n",
    "\n",
    "        Args:\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                                as an input of the actual neural network.\n",
    "            weights         :   dict containing keys which are layers matching the model's layers with values being weight matrices \n",
    "                                matching the dimensions of the weight matrices of the model.\n",
    "            biases          :   dict containing keys which are layers matching the model's layers with values being bias matrices \n",
    "                                matching the dimensions of the bias matrices of the model.\n",
    "        '''\n",
    "        x = input_layer\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = np.matmul(x, weights[f'layer {i + 1}']) + biases[f'layer {i + 1}']\n",
    "            x = self.activation_functions[i][0](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def accuracy(self, true_labels, input_layer):\n",
    "        '''\n",
    "        This function outputs the accuracy of the model given an input and a target output.\n",
    "\n",
    "        Args:\n",
    "            true_labels     :   ndarray with shape (n, q) where q=self.layers[-1], i.e., true_labels has the same shape \n",
    "                                as an output of the actual neural network.\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                                as an input of the actual neural network.\n",
    "        '''\n",
    "        # If the output layer is softmax, then the variable is categorical, so we find the argmax's and calculate\n",
    "        if self.activation_function_name_list[len(self.layers) - 2] == 'softmax':\n",
    "            # Compute the predictions of the network and find the argmax value for each row\n",
    "            output_layer = self.__call__(input_layer)\n",
    "            predictions = np.argmax(output_layer, axis = 1)\n",
    "\n",
    "            # Find the argmax value for each row of the true labels\n",
    "            true_labs = np.argmax(true_labels, axis = 1)\n",
    "\n",
    "            # Create a boolean array and compute the accuracy\n",
    "            acc = (predictions == true_labs)\n",
    "            acc = np.sum(acc)/acc.shape[0]\n",
    "            return acc\n",
    "        \n",
    "        # If the output layer has sigmoid neurons, then the rounded output layer vector must exactly match the true label vector\n",
    "        elif self.activation_function_name_list[len(self.layers) - 2] == 'sigmoid':\n",
    "            # Compute the predictions of the network and find the rounded values for each row\n",
    "            output_layer = self.__call__(input_layer)\n",
    "            predictions = np.round(output_layer)\n",
    "\n",
    "            # Compute the accuracy via boolean calculations and return the final value\n",
    "            acc = (predictions == true_labels) - 1\n",
    "            acc = np.sum(acc, axis = 1)\n",
    "            acc = (acc == 0)\n",
    "            acc = np.sum(acc)/acc.shape[0]\n",
    "            return acc\n",
    "    \n",
    "\n",
    "\n",
    "    def neuron_activations(self, input_layer):\n",
    "        '''\n",
    "        This function will output the neuron activations and pre-activations for each layer in the network.\n",
    "\n",
    "        Args:\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                            as an input of the actual neural network.\n",
    "        '''\n",
    "        a_dict = {}     # Dictionary storing the neuron activations\n",
    "        z_dict = {}     # Dictionary storing the pre-activation function neuron activations\n",
    "        x = input_layer\n",
    "\n",
    "        # Add the input layer to a_dict for future use\n",
    "        a_dict['layer 0'] = x\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = np.matmul(x, self.weights[f'layer {i + 1}']) + self.biases[f'layer {i + 1}']\n",
    "            z_dict[f'layer {i + 1}'] = x\n",
    "            x = self.activation_functions[i][0](x)\n",
    "            a_dict[f'layer {i + 1}'] = x\n",
    "        return a_dict, z_dict\n",
    "    \n",
    "\n",
    "    \n",
    "    def derivatives(self, input_layer, true_labels, loss, n_training_set, lambd=0):\n",
    "        '''\n",
    "        This function computes the derivatives of the cost function w.r.t. each weight, bias, and neuron in the network \n",
    "        (except for the neurons corresponding to the input layer which is \"layer 0\").\n",
    "\n",
    "        Args:\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                                as an input of the actual neural network.\n",
    "            true_labels     :   ndarray with shape (n, q) where q=self.layers[-1], i.e., true_labels has the same shape \n",
    "                                as an output of the actual neural network.\n",
    "            loss            :   The name of the type of loss to be used in the derivative calculation. For example, \n",
    "                                \"mean squared error\", \"binary cross entropy\", or \"categorical cross entropy\".\n",
    "            lambd           :   Regularization constant. The dafault is 0 corresponding to no regularization.\n",
    "            n_training_set  :   The total number of training examples in the training set.\n",
    "        '''\n",
    "        a_derivatives = {}      # Dictionary storing the derivatives of the cost w.r.t. the neurons for each layer\n",
    "        z_derivatives = {}      # Dictionary storing the derivatives of the cost w.r.t. the weighted sums for specific layers with softmax activation functions\n",
    "        w_derivatives = {}      # Dictionary storing the derivatives of the cost w.r.t. the weights for each layer\n",
    "        b_derivatives = {}      # Dictionary storing the derivatives of the cost w.r.t. the biases for each layer\n",
    "\n",
    "        # Calculate the neuron activations and preactivations\n",
    "        a_dict, z_dict = self.neuron_activations(input_layer)\n",
    "\n",
    "        # Calculate dJ/da for each neuron a in the output layer\n",
    "        a_derivatives[f'layer {len(self.layers) - 1}'] = loss_function_dict[loss][1](true_labels, a_dict[f'layer {len(self.layers) - 1}'])\n",
    "\n",
    "        # If the final layer is softmax, calculate dJ/dz using dJ/da and da/dz (the jacobian)\n",
    "        final = len(self.layers) - 1\n",
    "        if self.activation_function_name_list[final - 1] == 'softmax':\n",
    "            # Get dJ/da for layer i\n",
    "            dJ_da = a_derivatives[f'layer {final}'].reshape(input_layer.shape[0], a_derivatives[f'layer {final}'].shape[1], 1)\n",
    "\n",
    "            # Get the da/dz (the jacobian matrix) for layer i\n",
    "            da_dz = self.activation_functions[final - 1][1](z_dict[f'layer {final}'])\n",
    "\n",
    "            # Multiply the dJ/da and da/dz to get a (n,p,p) matrix whose column k (of the last index) sums to dJ/dz_k, when the first index is fixed\n",
    "            prod = dJ_da*da_dz\n",
    "            prod = np.sum(prod, axis = 1)\n",
    "\n",
    "            # Save these derivatives in the dictionary for weighted sum derivatives (z_derivatives)\n",
    "            z_derivatives[f'layer {final}'] = prod\n",
    "\n",
    "        # Calculate the derivative of the cost w.r.t. each neuron in the network\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            # Find dJ/da via the regular method if layer i + 1 is not softmax\n",
    "            if self.activation_function_name_list[i] != 'softmax':\n",
    "                # Get dJ/da for each neuron a in layer i + 1\n",
    "                dJ_da = a_derivatives[f'layer {i + 1}']\n",
    "\n",
    "                # Calculate the g'(z) values\n",
    "                g_prime_z = self.activation_functions[i][1](z_dict[f'layer {i + 1}'])\n",
    "\n",
    "                # Find the element-wise product of g'(z) and dJ/da\n",
    "                prod = (dJ_da*g_prime_z)\n",
    "\n",
    "                # Reshape the product so that it can broadcasted with NumPy and be multiplied elementwise with the weight matrix\n",
    "                prod = prod.reshape(input_layer.shape[0], 1, prod.shape[-1], order = 'C')\n",
    "\n",
    "                # Multiply the product with the weight matrix - if the second index is taken to be the rows (and third index is \n",
    "                # taken to be columns), the sum of row j will be the derivative of the cost w.r.t. a_j in layer i\n",
    "                prod = prod*self.weights[f'layer {i + 1}']\n",
    "\n",
    "                # Sum the rows to get a column vector for each data point whose jth entry is the derivative of the cost w.r.t. a_j\n",
    "                # in layer i\n",
    "                prod = np.sum(prod, axis = -1)\n",
    "\n",
    "                # Save these derivatives in the dictionary for neuron derivatives\n",
    "                a_derivatives[f'layer {i}'] = prod\n",
    "            \n",
    "            # Find dJ/da via dJ/dz for layer i + 1 and dz/da if layer i + 1 is softmax\n",
    "            elif self.activation_function_name_list[i] == 'softmax':\n",
    "                # Get dJ/da for each neuron a in layer i + 1\n",
    "                dJ_da = a_derivatives[f'layer {i + 1}']\n",
    "\n",
    "                # Calculate dz_da where z is from layer i + 1 and a is from layer i\n",
    "                # dJ/dz layer i + 1\n",
    "                prod = z_derivatives[f'layer {i + 1}'].reshape(input_layer.shape[0], 1, z_derivatives[f'layer {i + 1}'].shape[-1], order = 'C')\n",
    "                prod = prod*self.weights[f'layer {i + 1}']\n",
    "                prod = np.sum(prod, axis = -1)\n",
    "\n",
    "                # Save these derivatives in the dictionary for neuron derivatives\n",
    "                a_derivatives[f'layer {i}'] = prod\n",
    "            \n",
    "            # If layer i is softmax, calculate dJ/dz for layer i using dJ/da for layer i and da/dz (the jacobian matrix from the softmax derivative) for layer i\n",
    "            if self.activation_function_name_list[i - 1] == 'softmax':\n",
    "                # Get dJ/da for layer i\n",
    "                dJ_da = a_derivatives[f'layer {i}'].reshape(input_layer.shape[0], a_derivatives[f'layer {i}'].shape[1], 1)\n",
    "\n",
    "                # Get the da/dz (the jacobian matrix) for layer i\n",
    "                da_dz = self.activation_functions[i - 1][1](z_dict[f'layer {i}'])\n",
    "\n",
    "                # Multiply the dJ/da and da/dz to get a (n,p,p) matrix whose column k (the last index) sums to dJ/dz_k, when the first index is fixed\n",
    "                prod = dJ_da*da_dz\n",
    "                prod = np.sum(prod, axis = 1)\n",
    "\n",
    "                # Save these derivatives in the dictionary for weighted sum derivatives (z_derivatives)\n",
    "                z_derivatives[f'layer {i}'] = prod\n",
    "\n",
    "        # Calculate the derivative of the cost w.r.t. each weight in the network\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            # Find dJ/dw and dJ/db via the regular method if layer i is not softmax\n",
    "            if self.activation_function_name_list[i - 1] != 'softmax':\n",
    "                # Calculate the g'(z) values\n",
    "                g_prime_z = self.activation_functions[i - 1][1](z_dict[f'layer {i}'])\n",
    "\n",
    "                # Get dJ/da for each neuron a in layer i\n",
    "                dJ_da = a_derivatives[f'layer {i}']\n",
    "\n",
    "                # Find the element-wise product of g'(z) and dJ/da\n",
    "                prod = (g_prime_z*dJ_da)\n",
    "\n",
    "                # Reshape the product so that it can broadcasted with NumPy and be multiplied elementwise with the weight matrix\n",
    "                prod = prod.reshape(input_layer.shape[0], 1, prod.shape[-1], order = 'C')\n",
    "\n",
    "                # Save these derivatives in the dictionary for bias derivatives since this is it\n",
    "                b_derivatives[f'layer {i}'] = prod\n",
    "                \n",
    "                # Get previous layer activations\n",
    "                acts = a_dict[f'layer {i - 1}']\n",
    "\n",
    "                # Reshape the previous layer activations to (n, p, 1) where n is the number of data points and p is the number of \n",
    "                # neurons in layer i - 1\n",
    "                acts = acts.reshape(acts.shape[0], acts.shape[-1], 1, order = 'C')\n",
    "\n",
    "                # Multiply the matrices elementwise, exploiting NumPy's broadcasting rules\n",
    "                prod = prod*acts\n",
    "\n",
    "                # Save these derivatives in the dictionary for weight derivatives\n",
    "                w_derivatives[f'layer {i}'] = prod\n",
    "\n",
    "            # Find dJ/dw and dJ/db via dJ/dz if layer i is softmax\n",
    "            elif self.activation_function_name_list[i - 1] == 'softmax':\n",
    "                # Get dJ/dz from the z_derivatives dictionary\n",
    "                dJ_dz = z_derivatives[f'layer {i}'].reshape(input_layer.shape[0], 1, z_derivatives[f'layer {i}'].shape[-1], order = 'C')\n",
    "\n",
    "                # Save these derivatives in the dictionary for bias derivatives\n",
    "                b_derivatives[f'layer {i}'] = dJ_dz\n",
    "\n",
    "                # Get dz/dW (which is just the activation for layer i - 1) from a_dict\n",
    "                dz_dW = a_dict[f'layer {i - 1}'].reshape(input_layer.shape[0], a_dict[f'layer {i - 1}'].shape[-1], 1, order = 'C')\n",
    "\n",
    "                # Multiply dJ/dz and dz/dW, exploiting numpy broadcasting rules to get an array of shape (n, p, q)\n",
    "                prod = dJ_dz*dz_dW\n",
    "\n",
    "                # Save these derivatives in the dictionary for weight derivatives\n",
    "                w_derivatives[f'layer {i}'] = prod\n",
    "\n",
    "        # Add the regularization derivatives to the derivatives of the cost w.r.t. each weight in the network\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            w_derivatives[f'layer {i}'] += (lambd/n_training_set)*self.weights[f'layer {i}']\n",
    "\n",
    "        # Return the dictionaries containing the derivatives\n",
    "        return a_derivatives, w_derivatives, b_derivatives\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_check(self, input_row, true_label_row, loss, n_training_set, lambd=0, delta = 10**(-7)):\n",
    "        # Compute the actual derivatives (we only need w_derivatives and b_derivatives)\n",
    "        a_derivatives, w_derivatives, b_derivatives = self.derivatives(input_row, true_label_row, loss, n_training_set, lambd)\n",
    "\n",
    "        # Create dictionaries to store the gradient estimates\n",
    "        weight_grad_estimates = {}\n",
    "        bias_grad_estimates = {}\n",
    "\n",
    "        # Populate the gradient estmimate dictionaries with \"zero\" matrices of the same shape as the weight matrices for the respective layers\n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            weight_grad_estimates[f'layer {i}'] = np.zeros_like(self.weights[f'layer {i}'], dtype = float)\n",
    "            bias_grad_estimates[f'layer {i}'] = np.zeros_like(self.biases[f'layer {i}'], dtype = float)\n",
    "\n",
    "        # Iterate through weights and biases to compute the gradient estimates w.r.t. the individual weights and biases\n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            # Weights\n",
    "            for j in range(self.weights[f'layer {i}'].shape[0]):\n",
    "                for k in range(self.weights[f'layer {i}'].shape[1]):\n",
    "                    # Modify the model's weight matrix to get two more weight matrices from which the gradient estimate will be calculated\n",
    "                    w_delta_lower = self.weights[f'layer {i}'].copy()\n",
    "                    w_delta_upper = self.weights[f'layer {i}'].copy()\n",
    "\n",
    "                    w_delta_lower[j,k] -= delta\n",
    "                    w_delta_upper[j,k] += delta\n",
    "\n",
    "                    # Duplicate the weights of the model and modify them accordingly\n",
    "                    w_delta_lower_full = self.weights.copy()\n",
    "                    w_delta_lower_full[f'layer {i}'] = w_delta_lower\n",
    "                    w_delta_upper_full = self.weights.copy()\n",
    "                    w_delta_upper_full[f'layer {i}'] = w_delta_upper\n",
    "\n",
    "                    # Compute the regularization sums\n",
    "                    regularization_lower = (lambd/(2*n_training_set))*np.sum([np.sum(w_delta_lower_full[f'layer {ii}']**2) for ii in range(len(self.layers) - 1, 0, -1)])\n",
    "                    regularization_upper = (lambd/(2*n_training_set))*np.sum([np.sum(w_delta_upper_full[f'layer {ii}']**2) for ii in range(len(self.layers) - 1, 0, -1)])\n",
    "\n",
    "                    # Compute the costs and gradient estimate with regularization\n",
    "                    cost_lower = np.sum(loss_function_dict[loss][0](true_label_row, self.call_with_parameters(input_row, w_delta_lower_full, self.biases))) + regularization_lower\n",
    "                    cost_upper = np.sum(loss_function_dict[loss][0](true_label_row, self.call_with_parameters(input_row, w_delta_upper_full, self.biases))) + regularization_upper\n",
    "                    derivative_estimate = (cost_upper - cost_lower)/(2*delta)\n",
    "\n",
    "                    # Store the gradient estimate in the weight_grad_estimates dictionary\n",
    "                    weight_grad_estimates[f'layer {i}'][j,k] = derivative_estimate\n",
    "\n",
    "            # Biases\n",
    "            for j in range(self.biases[f'layer {i}'].shape[1]):\n",
    "                # Modify the model's weight matrix to get two more weight matrices from which the gradient estimate will be calculated\n",
    "                b_delta_lower = self.biases[f'layer {i}'].copy()\n",
    "                b_delta_upper = self.biases[f'layer {i}'].copy()\n",
    "\n",
    "                b_delta_lower[0,j] -= delta\n",
    "                b_delta_upper[0,j] += delta\n",
    "\n",
    "                # Duplicate the biases of the model and modify them accordingly\n",
    "                b_delta_lower_full = self.biases.copy()\n",
    "                b_delta_lower_full[f'layer {i}'] = b_delta_lower\n",
    "                b_delta_upper_full = self.biases.copy()\n",
    "                b_delta_upper_full[f'layer {i}'] = b_delta_upper\n",
    "\n",
    "                # Compute the regularization sums\n",
    "                regularization_lower = (lambd/(2*n_training_set))*np.sum([np.sum(self.weights[f'layer {ii}']**2) for ii in range(len(self.layers) - 1, 0, -1)])\n",
    "                regularization_upper = (lambd/(2*n_training_set))*np.sum([np.sum(self.weights[f'layer {ii}']**2) for ii in range(len(self.layers) - 1, 0, -1)])\n",
    "\n",
    "                # Compute the costs and gradient estimate\n",
    "                cost_lower = np.sum(loss_function_dict[loss][0](true_label_row, self.call_with_parameters(input_row, self.weights, b_delta_lower_full))) + regularization_lower\n",
    "                cost_upper = np.sum(loss_function_dict[loss][0](true_label_row, self.call_with_parameters(input_row, self.weights, b_delta_upper_full))) + regularization_upper\n",
    "                derivative_estimate = (cost_upper - cost_lower)/(2*delta)\n",
    "\n",
    "                # Store the gradient estimate in the weight_grad_estimates dictionary\n",
    "                bias_grad_estimates[f'layer {i}'][0,j] = derivative_estimate\n",
    "\n",
    "        # Now all of the estimated gradients are stored. Next, all derivatives will be put into a long vector and gradient estimates will \n",
    "        # be put into a long vector. These will be compared to determine whether the computed derivative is the actual gradient.\n",
    "        \n",
    "        # Assemble the long vector for the computed derivative and the gradient estimate\n",
    "        # Weights\n",
    "        long_computed_derivatives_w = np.concatenate([w_derivatives[f'layer {i}'][0].reshape(-1,) for i in range(1, len(self.layers) - 1)])\n",
    "        long_gradient_estimates_w = np.concatenate([weight_grad_estimates[f'layer {i}'].reshape(-1,) for i in range(1, len(self.layers) - 1)])\n",
    "\n",
    "        # Biases\n",
    "        long_computed_derivatives_b = np.concatenate([b_derivatives[f'layer {i}'][0].reshape(-1,) for i in range(1, len(self.layers) - 1)])\n",
    "        long_gradient_estimates_b = np.concatenate([bias_grad_estimates[f'layer {i}'].reshape(-1,) for i in range(1, len(self.layers) - 1)])\n",
    "\n",
    "        # Combination\n",
    "        long_computed_derivatives = np.concatenate([long_computed_derivatives_w, long_computed_derivatives_b])\n",
    "        long_gradient_estimates = np.concatenate([long_gradient_estimates_w, long_gradient_estimates_b])\n",
    "\n",
    "        # Compute the gradient check value\n",
    "        grad_check_value = np.linalg.norm(long_computed_derivatives - long_gradient_estimates)/(np.linalg.norm(long_computed_derivatives) + np.linalg.norm(long_gradient_estimates))\n",
    "\n",
    "        # Print the gradient check value\n",
    "        print(f'Gradient check value: {grad_check_value}')\n",
    "        \n",
    "        # Find cos(theta), where theta is the angle between the two derivative vectors in n-dimensional space.\n",
    "        # If cos(theta) is near 1, then the angle between the derivative vectors is very close to 0, which indicates that they point in very similar directions.\n",
    "        v = long_computed_derivatives\n",
    "        w = long_gradient_estimates\n",
    "        cos_theta = np.dot(v,w)/(np.linalg.norm(v)*np.linalg.norm(w))\n",
    "        print(f'cos(theta)={cos_theta}\\nv magnitude={np.linalg.norm(v)}, w magnitude={np.linalg.norm(w)}')\n",
    "        return grad_check_value, cos_theta, (v==0).all(), (w==0).all()\n",
    "\n",
    "\n",
    "\n",
    "    def step_size(self, optimizer, step_number, input_layer, true_labels, loss, n_training_set, lambd=0, previous_v = (0,0), previous_s = (0,0), beta_1 = 0.9, beta_2 = 0.999, epsilon = 10**(-8)):\n",
    "        '''\n",
    "        This function computes the step size for a given optimizer (before multiplying by the learning rate).\n",
    "        \n",
    "        Args:\n",
    "            optimizer       :   Must be one of \"sgd\", \"momentum\", \"rmsprop\", or \"adam\".\n",
    "            step_number     :   The number corresponding to how many updates have occurred since the of the v and s values\n",
    "                                used in the optimizer calculations.\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                                as an input of the actual neural network.\n",
    "            true_labels     :   ndarray with shape (n, q) where q=self.layers[-1], i.e., true_labels has the same shape \n",
    "                                as an output of the actual neural network.\n",
    "            loss            :   The name of the type of loss to be used in the derivative calculation. For example, \n",
    "                                \"mean squared error\", \"binary cross entropy\", or \"categorical cross entropy\".\n",
    "            previous_v      :   Tuple containing a dict containing the last v values computed for the \"momentum\" and \n",
    "                                \"adam\" optimizers for weights at index 0 and for biases at index 1.\n",
    "            previous_s      :   Tuple containing a dict containing the last s values computed for the \"rmsprop\" and \n",
    "                                \"adam\" optimizers for weights at index 0 and for biases at index 1.\n",
    "            beta_1          :   The constant beta used for the computation of v values for \"momentum\" and \"adam\".\n",
    "            beta_2          :   The constant beta used for the computation of v values for \"rmsprop\" and \"adam\".\n",
    "            epsilon         :   A small value to be added to denominators in \"rmsprop\" and \"adam\" calculations to prevent \n",
    "                                division by 0.\n",
    "        '''\n",
    "        # Calculate derivatives\n",
    "        a_derivatives, w_derivatives, b_derivatives = self.derivatives(input_layer, true_labels, loss, n_training_set, lambd)\n",
    "\n",
    "        # Get previous step dictionaries\n",
    "        w_previous_v, b_previous_v = previous_v\n",
    "        w_previous_s, b_previous_s = previous_s\n",
    "\n",
    "        # Intialize dictionaries to store average gradients\n",
    "        w_derivatives_avg = {}\n",
    "        b_derivatives_avg = {}\n",
    "\n",
    "        # Compute the average gradient for the minibatch (ideally this for loop would be vectorized)\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            # Compute the average derivative of the cost w.r.t. the weights and biases in layer i, respectively\n",
    "            dJ_dw_avg = np.mean(w_derivatives[f'layer {i}'], axis = 0)\n",
    "            dJ_db_avg = np.mean(b_derivatives[f'layer {i}'], axis = 0)\n",
    "\n",
    "            # Store average derivatives in their respective dictionaries\n",
    "            w_derivatives_avg[f'layer {i}'] = dJ_dw_avg\n",
    "            b_derivatives_avg[f'layer {i}'] = dJ_db_avg\n",
    "\n",
    "        # Compute/select step sizes based on the optimizer input\n",
    "        if (optimizer =='sgd'):\n",
    "            step_sizes = w_derivatives_avg, b_derivatives_avg\n",
    "\n",
    "            return step_sizes\n",
    "\n",
    "        elif (optimizer == 'momentum'):\n",
    "            # IMPLEMENT MOMENTUM WITH BIAS CORRECTION HERE\n",
    "\n",
    "            bias_correction_value = 1 - beta_1**step_number\n",
    "\n",
    "            # Initialize dictionaries to store the next v values\n",
    "            v_dw = {}\n",
    "            v_db = {}\n",
    "\n",
    "            # Initialize dictionaries to store the next steps\n",
    "            step_w = {}\n",
    "            step_b = {}\n",
    "            \n",
    "            # Calculate the new steps v_dw and v_db for each layer i\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                # Calculate the momentum terms\n",
    "                v_dw[f'layer {i}'] = beta_1*w_previous_v[f'layer {i}'] + (1 - beta_1)*w_derivatives_avg[f'layer {i}']\n",
    "                v_db[f'layer {i}'] = beta_1*b_previous_v[f'layer {i}'] + (1 - beta_1)*b_derivatives_avg[f'layer {i}']\n",
    "\n",
    "                # Calculate the step sizes with bias correction\n",
    "                step_w[f'layer {i}'] = v_dw[f'layer {i}']/bias_correction_value\n",
    "                step_b[f'layer {i}'] = v_db[f'layer {i}']/bias_correction_value\n",
    "\n",
    "            # Store the step_w, step_b, and v_dw, v_db dictionaries as tuples\n",
    "            step_sizes = step_w, step_b\n",
    "            v_values = v_dw, v_db\n",
    "\n",
    "            return step_sizes, v_values\n",
    "\n",
    "        elif (optimizer == 'rmsprop'):\n",
    "            # IMPLEMENT RMSPROP WITH BIAS CORRECTION\n",
    "\n",
    "            bias_correction_value = 1 - beta_2**step_number\n",
    "\n",
    "            # Initialize dictionaries to store the next s values\n",
    "            s_dw = {}\n",
    "            s_db = {}\n",
    "\n",
    "            # Initialize dictionaries to store the next steps\n",
    "            step_w = {}\n",
    "            step_b = {}\n",
    "            \n",
    "            # Calculate the new s_dw and s_db for each layer i then calculate the gradient step\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                # Calculate the 'mean squares'\n",
    "                s_dw[f'layer {i}'] = beta_2*w_previous_s[f'layer {i}'] + (1 - beta_2)*(w_derivatives_avg[f'layer {i}']**2)\n",
    "                s_db[f'layer {i}'] = beta_2*b_previous_s[f'layer {i}'] + (1 - beta_2)*(b_derivatives_avg[f'layer {i}']**2)\n",
    "\n",
    "                # Calculate the next steps with bias correction\n",
    "                step_w[f'layer {i}'] = w_derivatives_avg[f'layer {i}']/(np.sqrt(s_dw[f'layer {i}']/bias_correction_value) + epsilon)\n",
    "                step_b[f'layer {i}'] = b_derivatives_avg[f'layer {i}']/(np.sqrt(s_db[f'layer {i}']/bias_correction_value) + epsilon)\n",
    "                \n",
    "                if (s_db[f'layer {i}']**0.5 < 0).any():\n",
    "                    print('WHAT')\n",
    "\n",
    "\n",
    "            \n",
    "            # Organize the step sizes and s values into tuples before returning them\n",
    "            step_sizes = step_w, step_b\n",
    "            s_values = s_dw, s_db\n",
    "\n",
    "            return step_sizes, s_values\n",
    "\n",
    "\n",
    "        elif (optimizer == 'adam'):\n",
    "            # IMPLEMENT ADAM OPTIMIZER WITH BIAS CORRECTION\n",
    "\n",
    "            bias_correction_value_1 = 1 - beta_1**step_number\n",
    "            bias_correction_value_2 = 1 - beta_2**step_number\n",
    "\n",
    "            # Initialize dictionaries to store the next v values\n",
    "            v_dw = {}\n",
    "            v_db = {}\n",
    "\n",
    "            # Initialize dictionaries to store the next s values\n",
    "            s_dw = {}\n",
    "            s_db = {}\n",
    "\n",
    "            # Initialize dictionaries to store the next steps\n",
    "            step_w = {}\n",
    "            step_b = {}\n",
    "            \n",
    "            # Calculate the new steps v_dw and v_db for each layer i\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                # Calculate the momentum terms\n",
    "                v_dw[f'layer {i}'] = beta_1*w_previous_v[f'layer {i}'] + (1 - beta_1)*w_derivatives_avg[f'layer {i}']\n",
    "                v_db[f'layer {i}'] = beta_1*b_previous_v[f'layer {i}'] + (1 - beta_1)*b_derivatives_avg[f'layer {i}']\n",
    "\n",
    "                # Calculate the 'mean squares'\n",
    "                s_dw[f'layer {i}'] = beta_2*w_previous_s[f'layer {i}'] + (1 - beta_2)*(w_derivatives_avg[f'layer {i}']**2)\n",
    "                s_db[f'layer {i}'] = beta_2*b_previous_s[f'layer {i}'] + (1 - beta_2)*(b_derivatives_avg[f'layer {i}']**2)\n",
    "\n",
    "                # Calculate the next steps with bias corrections\n",
    "                step_w[f'layer {i}'] = (v_dw[f'layer {i}']/bias_correction_value_1)/(np.sqrt(s_dw[f'layer {i}']/bias_correction_value_2) + epsilon)\n",
    "                step_b[f'layer {i}'] = (v_db[f'layer {i}']/bias_correction_value_1)/(np.sqrt(s_db[f'layer {i}']/bias_correction_value_2) + epsilon)\n",
    "                \n",
    "                if (s_db[f'layer {i}'] < 0).any():\n",
    "                    print('WHAT')\n",
    "\n",
    "            # Organize the step sizes, v values and s values into tuples before returning them\n",
    "            step_sizes = step_w, step_b\n",
    "            v_values = v_dw, v_db\n",
    "            s_values = s_dw, s_db\n",
    "\n",
    "            return step_sizes, v_values, s_values\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_descent(self, alpha, batch_size, epochs, optimizer, input_layer, true_labels, loss, lambd, cv_input_layer = None, cv_true_labels = None, beta_1 = 0.9, beta_2 = 0.999, epsilon = 10**(-8)):\n",
    "        '''\n",
    "        This function applies gradient descent to the weights and biases of the \"neural\" instance. \n",
    "\n",
    "        Args:\n",
    "            alpha           :   The learning rate. Must be a float or int.\n",
    "            batch_size      :   The number of data points from which a gradient step will be calculated. Must be a \n",
    "                                positive int.\n",
    "            epochs          :   The number of times to go over the whole dataset to perform the gradient descent. Must be \n",
    "                                a positive int.\n",
    "            optimizer       :   The optimizer to be used in calculations. Must be one of \"sgd\", \"momentum\", \"rmsprop\", \n",
    "                                or \"adam\".\n",
    "            input_layer     :   ndarray with shape (n, p) where p=self.layers[0], i.e., input_layer has the same shape \n",
    "                                as an input of the actual neural network.\n",
    "            true_labels     :   ndarray with shape (n, q) where q=self.layers[-1], i.e., true_labels has the same shape \n",
    "                                as an output of the actual neural network.\n",
    "            loss            :   The name of the type of loss to be used in the derivative calculation. For example, \n",
    "                                \"mean squared error\", \"binary cross entropy\", or \"categorical cross entropy\".\n",
    "            lambd           :   const. The regularization lambda value. This value should be positive.\n",
    "            cv_input_layer  :   ndarray with shape (n_1, p) where p=self.layers[0], i.e., cv_input_layer has the same shape \n",
    "                                as an input of the actual neural network. If None, then no cv_accuracy or cv_loss will be printed or added to the history.\n",
    "            cv_true_labels  :   ndarray with shape (n_1, q) where q=self.layers[-1], i.e., cv_true_labels has the same shape \n",
    "                                as an output of the actual neural network. If None, then no cv_accuracy or cv_loss will be printed or added to the history.\n",
    "            previous_v      :   Tuple containing a dict containing the last v values computed for the \"momentum\" and \n",
    "                                \"adam\" optimizers for weights at index 0 and for biases at index 1.\n",
    "            previous_s      :   Tuple containing a dict containing the last s values computed for the \"rmsprop\" and \n",
    "                                \"adam\" optimizers for weights at index 0 and for biases at index 1.\n",
    "            beta_1          :   The constant beta used for the computation of v values for \"momentum\" and \"adam\".\n",
    "            beta_2          :   The constant beta used for the computation of v values for \"rmsprop\" and \"adam\".\n",
    "            epsilon         :   A small value to be added to denominators in \"rmsprop\" and \"adam\" calculations to prevent \n",
    "                                division by 0.\n",
    "        '''\n",
    "        # Create dictionaries to store v_values and s_values for both the weights and biases\n",
    "        w_v_values = {}\n",
    "        b_v_values = {}\n",
    "        \n",
    "        w_s_values = {}\n",
    "        b_s_values = {}\n",
    "\n",
    "        # Initialize the v_dw, v_db, s_dw and s_db to 0 for each layer i\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            w_v_values[f'layer {i}'] = np.zeros_like(self.weights[f'layer {i}'], dtype=float)\n",
    "            b_v_values[f'layer {i}'] = np.zeros_like(self.biases[f'layer {i}'], dtype=float)\n",
    "\n",
    "            w_s_values[f'layer {i}'] = np.zeros_like(self.weights[f'layer {i}'], dtype=float)\n",
    "            b_s_values[f'layer {i}'] = np.zeros_like(self.biases[f'layer {i}'], dtype=float)\n",
    "\n",
    "        # Initialize the step number t which will be used to calculate the bias correction term (1-beta^t) for the optimizers\n",
    "        step = 1\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data (CHECK THIS STEP TO SEE IF IT RUINS THE GRADIENT DESCENT)\n",
    "            permutation = np.random.permutation(input_layer.shape[0])\n",
    "            xx = input_layer[permutation]\n",
    "            yy = true_labels[permutation]\n",
    "\n",
    "            # Perform minibatch gradient descent using minibatches of size batch_size\n",
    "            for j in range(-(-len(xx)//batch_size)):\n",
    "                # Get a minibatch\n",
    "                minibatch_x = xx[j : min(j + batch_size, xx.shape[0])]\n",
    "                minibatch_y = yy[j : min(j + batch_size, yy.shape[0])]\n",
    "\n",
    "                previous_v_ = w_v_values, b_v_values\n",
    "                previous_s_ = w_s_values, b_s_values\n",
    "\n",
    "                # Calculate the gradient steps, v_values and s_values\n",
    "                step_sizes_output = self.step_size(optimizer, step, minibatch_x, minibatch_y, loss, input_layer.shape[0], lambd, previous_v = previous_v_, previous_s = previous_s_, beta_1 = beta_1, beta_2 = beta_2, epsilon = epsilon)\n",
    "                \n",
    "                # Update the step number for the next update\n",
    "                step += 1\n",
    "\n",
    "                # Separate the actual step sizes from the rest of the outputs in step_sizes_outputs\n",
    "                if optimizer == 'sgd':\n",
    "                    w_step_sizes, b_step_sizes = step_sizes_output\n",
    "                elif optimizer == 'momentum':\n",
    "                    w_step_sizes, b_step_sizes = step_sizes_output[0]\n",
    "                    w_v_values, b_v_values = step_sizes_output[1]\n",
    "                elif optimizer == 'rmsprop':\n",
    "                    w_step_sizes, b_step_sizes = step_sizes_output[0]\n",
    "                    w_s_values, b_s_values = step_sizes_output[1]\n",
    "                elif optimizer == 'adam':\n",
    "                    w_step_sizes, b_step_sizes = step_sizes_output[0]\n",
    "                    w_v_values, b_v_values = step_sizes_output[1]\n",
    "                    w_s_values, b_s_values = step_sizes_output[2]\n",
    "\n",
    "                # Update the weights and biases\n",
    "                for k in range(len(self.layers) - 1, 0, -1):\n",
    "                    self.weights[f'layer {k}'] -= alpha * w_step_sizes[f'layer {k}']\n",
    "                    self.biases[f'layer {k}'] -= alpha * b_step_sizes[f'layer {k}']\n",
    "\n",
    "                # Every so often, store the history of the weights and biases\n",
    "                if j%10 == 0:\n",
    "                    # Initialize list to store max absolute gradients for each layer to see whether gradients are vanishing or exploding\n",
    "                    w_grad_max = []\n",
    "                    for k in range(len(self.layers) - 1, 0, -1):\n",
    "                        w_grad_max.insert(0, np.max(np.abs(w_step_sizes[f'layer {k}'])))\n",
    "\n",
    "                    # Initialize list to store max absolute gradients for each layer to see whether gradients are vanishing or exploding\n",
    "                    w_grad_mean = []\n",
    "                    for k in range(len(self.layers) - 1, 0, -1):\n",
    "                        w_grad_mean.insert(0, np.mean(np.abs(w_step_sizes[f'layer {k}'])))\n",
    "\n",
    "                    # First get the model predictions\n",
    "                    output_layer = self.__call__(input_layer)\n",
    "                    \n",
    "                    # Compute the cost and accuracy\n",
    "                    cost = np.mean(loss_function_dict[loss][0](true_labels, output_layer))\n",
    "                    acc = self.accuracy(true_labels, input_layer)\n",
    "\n",
    "                    # Save the history\n",
    "                    self.cost_history = np.append(self.cost_history, cost)\n",
    "                    self.accuracy_history = np.append(self.accuracy_history, acc)\n",
    "                    self.weight_update_max_history = np.vstack([self.weight_update_max_history, w_grad_max])\n",
    "                    self.weight_update_mean_history = np.vstack([self.weight_update_mean_history, w_grad_mean])\n",
    "\n",
    "                    # Compute and save the cross-validation cost and accuracy\n",
    "                    if (type(cv_input_layer) != type(None)) and (type(cv_true_labels) != type(None)):\n",
    "                        # Get neural network prediction\n",
    "                        cv_output_layer = self.__call__(cv_input_layer)\n",
    "\n",
    "                        # Compute cost and accuracy\n",
    "                        cv_cost = np.mean(loss_function_dict[loss][0](cv_true_labels, cv_output_layer))\n",
    "                        cv_acc = self.accuracy(cv_true_labels, cv_input_layer)\n",
    "\n",
    "                        # Save in the dictionaries\n",
    "                        self.cv_cost_history = np.append(self.cv_cost_history, cv_cost)\n",
    "                        self.cv_accuracy_history = np.append(self.cv_accuracy_history, cv_acc)\n",
    "\n",
    "                # Every so often, print feedback\n",
    "                if j%10 == 0:\n",
    "                    # Initialize list to store max absolute gradients for each layer to see whether gradients are vanishing or exploding\n",
    "                    w_grad_max = []\n",
    "                    for k in range(len(self.layers) - 1, 0, -1):\n",
    "                        w_grad_max.insert(0, np.max(np.abs(w_step_sizes[f'layer {k}'])))\n",
    "\n",
    "                    # Compute and print the average loss at the end of the epoch\n",
    "                    # First get the model predictions\n",
    "                    output_layer = self.__call__(input_layer)\n",
    "\n",
    "                    # Compute the cost\n",
    "                    cost = np.mean(loss_function_dict[loss][0](true_labels, output_layer))\n",
    "\n",
    "                    # Compute the accuracy\n",
    "                    acc = self.accuracy(true_labels, input_layer)\n",
    "                    \n",
    "                    # Print the stats\n",
    "                    print(f'Max absolute weight updates for each layer: {w_grad_max}\\nUnregularized cost at minibatch {j} of epoch {i} : {cost}\\nAccuracy at minibatch {j} epoch {i} : {acc}')\n",
    "                        \n",
    "            # Print the unregularized cost\n",
    "            if (i%1 == 0) or (i == epochs - 1):\n",
    "                # Compute and print the average loss at the end of the epoch\n",
    "                # First get the model predictions\n",
    "                predictions = self.__call__(input_layer)\n",
    "\n",
    "                # Compute the cost\n",
    "                cost = np.mean(loss_function_dict[loss][0](true_labels, predictions))\n",
    "\n",
    "                print(f'Unregularized cost at epoch {i} : {cost}')\n",
    "\n",
    "                # Compute the accuracy\n",
    "                acc = self.accuracy(true_labels, input_layer)\n",
    "                print(f'Accuracy at epoch {i} : {acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fcb8a",
   "metadata": {},
   "source": [
    "# Engineer the Data\n",
    "\n",
    "Since the data consists of handrwritten digits, the outputs are categorical. Thus, a softmax output layer will be ideal and the labels must be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146de4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to be used to train the model\n",
    "cutoff = 10000\n",
    "\n",
    "# Load data (you need to download the relevant dataset as a CSV, e.g. CIFAR-10, MNIST digits, or Fashion-MNIST)\n",
    "data = pd.read_csv(\"path_to_data\", nrows=cutoff*2)\n",
    "\n",
    "# Separate inputs and labels\n",
    "x_train = data[data.columns[:-1]]\n",
    "y_train = data[[data.columns[-1]]]\n",
    "\n",
    "# Put the data into arrays\n",
    "x_array = np.array(x_train)\n",
    "y_array = np.array(y_train)\n",
    "\n",
    "# Scale inputs by 1/255\n",
    "x_array = x_array/255\n",
    "\n",
    "# One-hot encoding of labels\n",
    "y_one_hot = np.array([[1 if y_array[i, 0] == j else 0 for j in range(10)] for i in range(y_array.shape[0])])\n",
    "\n",
    "# Data to be used to train the model\n",
    "xxx = x_array[:cutoff]\n",
    "yyy = y_one_hot[:cutoff]\n",
    "\n",
    "# Cross-validation data\n",
    "xxx_cv = x_array[cutoff:cutoff*2]\n",
    "yyy_cv = y_one_hot[cutoff:cutoff*2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c00e3",
   "metadata": {},
   "source": [
    "# Train a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb88baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters for gradient descent\n",
    "learning_rate_ = 0.001\n",
    "minibatch_size_ = 100\n",
    "epochs_ = 20\n",
    "optimizer_ = 'adam'\n",
    "inputs_ = xxx\n",
    "true_outputs_ = yyy\n",
    "loss_ = 'categorical cross entropy'\n",
    "cv_input_layer_ = xxx_cv\n",
    "cv_true_labels_ = yyy_cv\n",
    "lambd_ = 0\n",
    "beta_1_ = 0.9\n",
    "beta_2_ = 0.999\n",
    "epsilon_ = 10**(-8)\n",
    "\n",
    "# Select network architecture\n",
    "# Note that the activation functions are only for the layers after the first layer since the first layer is the input layer\n",
    "# Therefore, the activation function list must be one less than the layer dimensions list\n",
    "layer_dimensions = [xxx.shape[1], 128, 128, 128, 128, yyy.shape[1]]\n",
    "activation_functions = ['leaky relu', 'leaky relu', 'leaky relu', 'leaky relu', 'softmax']\n",
    "\n",
    "# Initialize neural network without\n",
    "nn = neural(layers=layer_dimensions, activation_function_list=activation_functions, he_uniform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent on the network\n",
    "nn.gradient_descent(learning_rate_, minibatch_size_, epochs_, optimizer_, inputs_, true_outputs_, loss_, lambd_, cv_input_layer = cv_input_layer_, cv_true_labels = cv_true_labels_, beta_1 = beta_1_, beta_2 = beta_2_, epsilon = epsilon_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d4f75",
   "metadata": {},
   "source": [
    "# Train Models for an Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0085acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of the variables to be included in the ablation study\n",
    "optimizer_choice = ['sgd', 'momentum', 'rmsprop', 'adam']\n",
    "optimizer_choice_lr = [0.01, 0.03, 0.001, 0.001] # These are not variables, but are the fixed learning rates associated with each optimizer\n",
    "he_initial_choice = [False, True]\n",
    "regularization_choice = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548dfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models for the ablation study\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Select the name of the file that the dictionary will be store in\n",
    "file_path = 'neural_dict.pkl'\n",
    "\n",
    "# Initialize the dictionary that will store all of the neural networks\n",
    "try:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        neural_network_dict = pickle.load(file)\n",
    "except:\n",
    "    neural_network_dict = {}\n",
    "\n",
    "# Load the last index started\n",
    "try:\n",
    "    checkpoint_index = np.load('checkpoint_index_i.npy')[0]\n",
    "except:\n",
    "    checkpoint_index = 0\n",
    "\n",
    "\n",
    "# Continue from the last saved checkpoint\n",
    "for i in range(checkpoint_index, len(optimizer_choice)):\n",
    "    # Save the current index so that progress can be resumed from a checkpoint\n",
    "    np.save('checkpoint_index_i.npy', np.array([i]))\n",
    "\n",
    "    # Initialize dictionary to store the neural networks\n",
    "    neural_network_dict[optimizer_choice[i]] = {}\n",
    "\n",
    "    # Loop over the rest of the variables\n",
    "    for j in range(len(he_initial_choice)):\n",
    "        neural_network_dict[optimizer_choice[i]][f'He Uniform {he_initial_choice[j]}'] = {}\n",
    "        for k in range(len(regularization_choice)):\n",
    "            # Select parameters for gradient descent\n",
    "            learning_rate_ = optimizer_choice_lr[i]\n",
    "            minibatch_size_ = 100\n",
    "            epochs_ = 1\n",
    "            optimizer_ = optimizer_choice[i]\n",
    "            inputs_ = xxx\n",
    "            true_outputs_ = yyy\n",
    "            loss_ = 'categorical cross entropy'\n",
    "            lambd_ = regularization_choice[k]\n",
    "            cv_input_layer_ = xxx_cv\n",
    "            cv_true_labels_ = yyy_cv\n",
    "            beta_1_ = 0.9\n",
    "            beta_2_ = 0.999\n",
    "            epsilon_ = 10**(-8)\n",
    "            \n",
    "            # Select network structure\n",
    "            layer_dimensions = [xxx.shape[1], 128, 128, 128, 128, yyy.shape[1]]\n",
    "            activation_functions = ['leaky relu', 'leaky relu', 'leaky relu', 'leaky relu', 'softmax']\n",
    "\n",
    "            # Initialize neural network with the choice of He initialization or not\n",
    "            nn = neural(layers=layer_dimensions, activation_function_list=activation_functions, he_uniform=he_initial_choice[j])\n",
    "\n",
    "            neural_network_dict[optimizer_choice[i]][f'He Uniform {he_initial_choice[j]}'][f'Regularization lambd {regularization_choice[k]}'] = nn\n",
    "\n",
    "            # Train neural network\n",
    "            nn.gradient_descent(learning_rate_, minibatch_size_, epochs_, optimizer_, inputs_, true_outputs_, loss_, lambd_, cv_input_layer=cv_input_layer_, cv_true_labels=cv_true_labels_, beta_1 = beta_1_, beta_2 = beta_2_, epsilon = epsilon_)\n",
    "\n",
    "            # Update the file containing the dictionary\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(neural_network_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059cecb",
   "metadata": {},
   "source": [
    "# Load Models for the Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d776955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the neural_network_dictionary into a numpy array to enable easy isolation of variables to conduct an ablation study effectively\n",
    "import pickle\n",
    "\n",
    "# Load the dictionary containing the neural networks\n",
    "file_path = 'neural_dict.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    neural_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array for objects that contains the string 'a' as placeholders\n",
    "neural_array = np.full(shape=(len(optimizer_choice), len(he_initial_choice), len(regularization_choice)), fill_value='a', dtype=object)\n",
    "'''\n",
    "Indices (from left most index):\n",
    "\n",
    "    - Optimizer choice: \"sgd\" [0], \"momentum\" [1], \"rmsprop\" [2], \"adam\" [3]\n",
    "    - He initialization choice: False [0], True [1]\n",
    "    - Regularization choice: lambd=0 [0], lambd=1 [1], lambd=2 [2], lambd=3 [3]\n",
    "'''\n",
    "\n",
    "# Populate the neural_array with the relevant neural network at each index\n",
    "for i in range(len(optimizer_choice)):\n",
    "    for j in range(len(he_initial_choice)):\n",
    "        for k in range(len(regularization_choice)):\n",
    "            neural_array[i,j,k] = neural_dict[optimizer_choice[i]][f'He Uniform {he_initial_choice[j]}'][f'Regularization lambd {regularization_choice[k]}']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
